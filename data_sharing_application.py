# -*- coding: utf-8 -*-
"""data_sharing_application.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hj1UynENR0YyOjRRBipfA4UWXFF5XnBP
"""

seed=1
import os
os.environ['PYTHONHASHSEED'] = str(seed)
# For working on GPUs from "TensorFlow Determinism"
os.environ["TF_DETERMINISTIC_OPS"] = str(seed)
import numpy as np
np.random.seed(seed)
import random
random.seed(seed)
import tensorflow as tf
tf.random.set_seed(seed)
from google.colab import drive
from google.colab import files
import tensorflow as tf
import pandas as pd
import statistics
from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor
from sklearn.datasets import make_classification
from sklearn.model_selection import GridSearchCV, cross_validate
from sklearn.metrics import average_precision_score, log_loss, roc_curve, brier_score_loss, auc, accuracy_score, f1_score, roc_auc_score, recall_score, precision_recall_curve
from scipy import stats
import math
from google.colab import drive
import pandas as pd
import numpy as np
import statistics
from sklearn import metrics
import math
import matplotlib.pyplot as plt
from __future__ import print_function, division
from keras.datasets import mnist
from tensorflow.keras.optimizers import RMSprop
from functools import partial
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
import argparse
import random
import seaborn as sns
import os
import numpy as np
import keras
from tensorflow.keras import backend as K
from functools import partial
from google.colab import drive
from google.colab import files
import time
from __future__ import print_function, division
import os
from sklearn.linear_model import LinearRegression
import sys
import numpy as np
import matplotlib.pyplot as plt
from keras.layers import Input, Dense, Reshape, Flatten, Dropout
from keras.layers import BatchNormalization, Activation, ZeroPadding2D
from keras.layers import Input, Dense, Reshape, Flatten, Dropout, LSTM
from keras.layers import LeakyReLU
from keras.layers.convolutional import UpSampling2D, Conv2D, Conv1D
from keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import pandas as pd
import io
from keras.models import load_model
import time
from scipy.stats import pearsonr
from keras.datasets import mnist
from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise
from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D
from keras.layers import MaxPooling2D, LeakyReLU
from keras.layers.convolutional import UpSampling2D, Conv2D
from keras.models import Sequential, Model
from keras import losses
import keras.backend as K
import random
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KernelDensity
seed=1
import os
from sklearn.model_selection import train_test_split
os.environ['PYTHONHASHSEED'] = str(seed)
# For working on GPUs from "TensorFlow Determinism"
os.environ["TF_DETERMINISTIC_OPS"] = str(seed)
import numpy as np
np.random.seed(seed)
import random
random.seed(seed)
import tensorflow as tf
tf.random.set_seed(seed)
print(random.random())

drive.mount('/content/drive')

"""# real data"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

data = pd.read_csv('department1.csv', sep = ',', na_values=['(NA)']).fillna(0).sample(frac = 1)
y_train = data.iloc[:,0]
X_train_department1 = data.iloc[:,1:5]
print(X_train_department1)
X_train_department2 = pd.read_csv('department2.csv', sep = ',', na_values=['(NA)']).fillna(0).sample(frac = 1)
print(X_train_department2)

test = pd.read_csv('test_pred.csv', sep = ',', na_values=['(NA)']).fillna(0).sample(frac = 1)
y_test = test.iloc[:,0]
X_test_department1 = test.iloc[:,1:5]
X_test_department2 = test.iloc[:,5:10]

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)
# n is sample to speed up
n = 1000

# obtain optimal params
clf = RandomForestClassifier(random_state=0, n_jobs = -1)
param_grid = {
    'class_weight': [None,"balanced", "balanced_subsample"],
    'n_estimators': [50, 100, 200,500],
    'max_features' : ['sqrt', 'log2', None],
    'max_depth': [1,5,None]
}
clf_grid = GridSearchCV(clf, param_grid, scoring = "f1", cv=2, n_jobs = -1)
clf_grid.fit(X_train_department1.sample(n), y_train.sample(n))

print(clf_grid.best_params_)

clf_test = clf_grid.best_estimator_
clf_test.fit(X_train_department1, y_train)

#predictions
predictions = clf_test.predict_proba(X_test_department1)
dataset = pd.DataFrame({'truth': y_test, 'pred': predictions[:,1]}, columns=['truth', 'pred'])
sorted = dataset.sort_values('pred', ascending = False)
top = sorted.nlargest(n = 1000, columns = 'pred')
percentage_real = sum(top['truth'])/1000
print(percentage_real*1000)

print(f1_score(y_test, clf_test.predict(X_test_department1)))

"""## combine data"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

train = pd.read_csv('train_pred.csv', sep = ',', na_values=['(NA)']).fillna(0)
X_train = train.iloc[:,1:10]
y_train = train.iloc[:,0]

test = pd.read_csv('test_pred.csv', sep = ',', na_values=['(NA)']).fillna(0).sample(frac = 1)
y_test = test.iloc[:,0]
X_test = test.iloc[:,1:10]

# n is sample to speed up
n = 1000

# obtain optimal params
clf = RandomForestClassifier(random_state=0, n_jobs = -1)
param_grid = {
    'class_weight': [None,"balanced", "balanced_subsample"],
    'n_estimators': [50, 100, 200,500],
    'max_features' : ['sqrt', 'log2', None],
    'max_depth': [1,5,None]
}
clf_test_grid = GridSearchCV(clf, param_grid, scoring = "f1", cv=2, n_jobs = -1)
clf_test_grid.fit(X_train.sample(n), y_train.sample(n))
print(clf_test_grid.best_params_)

clf_test = clf_test_grid.best_estimator_
clf_test.fit(X_train, y_train)

#predictions

predictions = clf_test.predict_proba(X_test)
dataset = pd.DataFrame({'truth': y_test, 'pred': predictions[:,1]}, columns=['truth', 'pred'])
sorted = dataset.sort_values('pred', ascending = False)
top = sorted.nlargest(n = 1000, columns = 'pred')
percentage_real = sum(top['truth'])/1000
print(percentage_real*1000)

print(f1_score(y_test, clf_test.predict(X_test)))

"""# GAN without differential privacy"""

class GAN():
    def __init__(self, privacy, input_dim):
      self.img_rows = 1
      self.img_cols = input_dim
      self.img_shape = (self.img_cols,)
      self.latent_dim = (input_dim)

      optimizer = keras.optimizers.Adam()
      self.discriminator = self.build_discriminator()
      self.discriminator.compile(loss='binary_crossentropy',
                                 optimizer=optimizer,
                                 metrics=['accuracy'])

      # Build the generator
      self.generator = self.build_generator()

      # The generator takes noise as input and generates imgs
      z = Input(shape=(self.latent_dim,))
      img = self.generator(z)

      # For the combined model we will only train the generator
      self.discriminator.trainable = False

      # The discriminator takes generated images as input and determines validity
      valid = self.discriminator(img)

      # The combined model  (stacked generator and discriminator)
      # Trains the generator to fool the discriminator
      self.combined = Model(z, valid)
      self.combined.compile(loss='binary_crossentropy', optimizer= optimizer)


    def build_generator(self):
      model = Sequential()
      model.add(Dense(self.latent_dim, input_dim=self.latent_dim))
      model.add(LeakyReLU(alpha=0.2))
      #model.add(BatchNormalization())
      model.add(Dense(4, input_shape=self.img_shape))
      model.add(LeakyReLU(alpha=0.2))
      #model.add(BatchNormalization())
      model.add(Dense(self.latent_dim))
      model.add(Activation("tanh"))

      #model.summary()

      noise = Input(shape=(self.latent_dim,))
      img = model(noise)
      return Model(noise, img)

    def build_discriminator(self):

        model = Sequential()

        model.add(Dense(4, input_shape=self.img_shape))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dense(1, activation='sigmoid'))

        #model.summary()

        img = Input(shape=self.img_shape)
        validity = model(img)

        return Model(img, validity)

    def train(self, data, iterations, batch_size, sample_interval, model_name, generator_losses = [], discriminator_acc = [], correlations = [], accuracy = [], MAPD_collect = [],MSE_collect = [], MAE_collect = []):
      # Adversarial ground truths
      valid = np.ones((batch_size, 1))
      fake = np.zeros((batch_size, 1))
      corr = 0
      MAPD = 0
      MSE = 0
      MAE = 0
      #fake += 0.05 * np.random.random(fake.shape)
      #valid += 0.05 * np.random.random(valid.shape)

      for epoch in range(iterations):

            # ---------------------
            #  Train Discriminator
            # ---------------------

            # Select a random batch of images
            idx = np.random.randint(0, data.shape[0], batch_size)
            imgs = data[idx]

            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))

            # Generate a batch of new images
            gen_imgs = self.generator.predict(noise, verbose = False)

            # Train the discriminator
            d_loss_real = self.discriminator.train_on_batch(imgs, valid)
            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

            # ---------------------
            #  Train Generator
            # ---------------------
            # Train the generator (to have the discriminator label samples as valid)

            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
            g_loss = self.combined.train_on_batch(noise, valid)
            #print ("%d [D loss: %f, acc.: %.2f%%] [G loss: %f, corr: %f, MAPD: %f, MSE: %f, MAE: %f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss, corr, MAPD, MSE, MAE))

      print("save model")
      self.generator.save(model_name)

"""## firm 1"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

data = pd.read_csv('department1.csv', sep = ',', na_values=['(NA)']).fillna(0)
X_train_department1 = data.iloc[:,0:5]
#print(X_train_department1)
X_train_department2 = pd.read_csv('department2.csv', sep = ',', na_values=['(NA)']).fillna(0)
#print(X_train_department2)

N = len(X_train_department1)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department1)
train_GAN = scaler0.transform(X_train_department1)
train_GAN_department1 = pd.DataFrame(train_GAN)
#print(train_GAN_department1)

#gan_train = GAN(privacy = False, input_dim = 5)
#gan_train.train(data = np.array(train_GAN_department1), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department1.h5")

# Generate a batch of new customers
generator = load_model('train_department1.h5')
noise = np.random.normal(0, 1, (N, 5))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department1 = pd.DataFrame(gen_imgs.reshape(N, 5))
print(train_GAN_department1.head())

# n is sample to speed up
n = 1000

# obtain optimal params
clf = RandomForestClassifier(random_state=0, n_jobs = -1)
param_grid = {
    'class_weight': [None,"balanced", "balanced_subsample"],
    'n_estimators': [50, 100, 200,500],
    'max_features' : ['sqrt', 'log2', None],
    'max_depth': [1,5,None]
}
clf_grid = GridSearchCV(clf, param_grid, scoring = "f1", cv=2, n_jobs = -1)
clf_grid.fit(train_GAN_department1.iloc[:,1:5].sample(n), np.around(train_GAN_department1.iloc[:,0].sample(n)))

print(clf_grid.best_params_)

clf_GAN = clf_grid.best_estimator_
clf_GAN.fit(train_GAN_department1.iloc[:,1:5], np.around(train_GAN_department1.iloc[:,0]))

#predictions
predictions = clf_GAN.predict_proba(X_test_department1)
dataset = pd.DataFrame({'truth': y_test, 'pred': predictions[:,1]}, columns=['truth', 'pred'])
sorted = dataset.sort_values('pred', ascending = False)
top = sorted.nlargest(n = 1000, columns = 'pred')
percentage_real = sum(top['truth'])/1000
print(percentage_real*1000)

print(f1_score(y_test, clf_GAN.predict(X_test_department1)))

"""## firm 2"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

X_train_department2 = pd.read_csv('department2.csv', sep = ',', na_values=['(NA)']).fillna(0)
print(X_train_department2)

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department2)
train_GAN = scaler0.transform(X_train_department2)
train_GAN_department2 = pd.DataFrame(train_GAN)


N = len(X_train_department2)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

#gan_train = GAN(privacy = False, input_dim = 4)
#gan_train.train(data = np.array(train_GAN_department2), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department2.h5")

# Generate a batch of new customers
generator = load_model('train_department2.h5')
noise = np.random.normal(0, 1, (N, 4))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department2 = pd.DataFrame(gen_imgs.reshape(N, 4))
print(train_GAN_department2.head())

"""## combine and target


"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

concatenated = pd.concat([train_GAN_department1, train_GAN_department2], axis="columns")
concatenated.columns = train.columns
y_train = concatenated.iloc[:,0]
X_train = concatenated.iloc[:,1:100]
print(y_train)
print(X_train)

# n is sample to speed up
n = 1000

# obtain optimal params
clf = RandomForestClassifier(random_state=0, n_jobs = -1)
param_grid = {
    'class_weight': [None,"balanced", "balanced_subsample"],
    'n_estimators': [50, 100, 200,500],
    'max_features' : ['sqrt', 'log2', None],
    'max_depth': [1,5,None]
}
clf_grid = GridSearchCV(clf, param_grid, scoring = "f1", cv=2, n_jobs = -1)
clf_grid.fit(X_train.sample(n), np.round(y_train.sample(n)))

print(clf_grid.best_params_)

clf_GAN = clf_grid.best_estimator_
clf_GAN.fit(X_train, np.round(y_train))

test = pd.read_csv('test_pred.csv', sep = ',', na_values=['(NA)']).fillna(0).sample(frac = 1)
y_test = test.iloc[:,0]
X_test = test.iloc[:,1:10]

#predictions
predictions = clf_GAN.predict_proba(X_test)
dataset = pd.DataFrame({'truth': y_test, 'pred': predictions[:,1]}, columns=['truth', 'pred'])
sorted = dataset.sort_values('pred', ascending = False)
top = sorted.nlargest(n = 1000, columns = 'pred')
percentage_real = sum(top['truth'])/1000
print(percentage_real*1000)

print(f1_score(y_test, clf_GAN.predict(X_test)))

"""# differential privacy $ɛ = 13$."""

!pip install tensorflow_privacy --quiet
from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy
from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer, DPKerasAdamOptimizer
from tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy_lib import compute_dp_sgd_privacy

class GAN():
    def __init__(self, privacy, input_dim):
      self.img_rows = 1
      self.img_cols = input_dim
      self.img_shape = (self.img_cols,)
      self.latent_dim = (input_dim)
      lr = 0.001

      optimizer = keras.optimizers.Adam()
      self.discriminator = self.build_discriminator()
      self.discriminator.compile(loss='binary_crossentropy',
                                 optimizer=optimizer,
                                 metrics=['accuracy'])
      if privacy == True:
        print(noise_multiplier)
        print("using differential privacy")
        # Build and compile the discriminator
        self.discriminator = self.build_discriminator()
        self.discriminator.compile(optimizer=DPKerasAdamOptimizer(
            l2_norm_clip=4,
            noise_multiplier=noise_multiplier,
            num_microbatches=num_microbatches,
            learning_rate=lr),
            loss= tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE), metrics=['accuracy'])

      # Build the generator
      self.generator = self.build_generator()

      # The generator takes noise as input and generates imgs
      z = Input(shape=(self.latent_dim,))
      img = self.generator(z)

      # For the combined model we will only train the generator
      self.discriminator.trainable = False

      # The discriminator takes generated images as input and determines validity
      valid = self.discriminator(img)

      # The combined model  (stacked generator and discriminator)
      # Trains the generator to fool the discriminator
      self.combined = Model(z, valid)
      self.combined.compile(loss='binary_crossentropy', optimizer= optimizer)


    def build_generator(self):
      model = Sequential()
      model.add(Dense(self.latent_dim, input_dim=self.latent_dim))
      model.add(LeakyReLU(alpha=0.2))
      #model.add(BatchNormalization())
      model.add(Dense(4, input_shape=self.img_shape))
      model.add(LeakyReLU(alpha=0.2))
      #model.add(BatchNormalization())
      model.add(Dense(self.latent_dim))
      model.add(Activation("tanh"))

      #model.summary()

      noise = Input(shape=(self.latent_dim,))
      img = model(noise)
      return Model(noise, img)

    def build_discriminator(self):

        model = Sequential()

        model.add(Dense(4, input_shape=self.img_shape))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dense(1, activation='sigmoid'))

        #model.summary()

        img = Input(shape=self.img_shape)
        validity = model(img)

        return Model(img, validity)

    def train(self, data, iterations, batch_size, sample_interval, model_name, generator_losses = [], discriminator_acc = [], correlations = [], accuracy = [], MAPD_collect = [],MSE_collect = [], MAE_collect = []):
      # Adversarial ground truths
      valid = np.ones((batch_size, 1))
      fake = np.zeros((batch_size, 1))
      corr = 0
      MAPD = 0
      MSE = 0
      MAE = 0
      #fake += 0.05 * np.random.random(fake.shape)
      #valid += 0.05 * np.random.random(valid.shape)

      for epoch in range(iterations):

            # ---------------------
            #  Train Discriminator
            # ---------------------

            # Select a random batch of images
            idx = np.random.randint(0, data.shape[0], batch_size)
            imgs = data[idx]

            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))

            # Generate a batch of new images
            gen_imgs = self.generator.predict(noise, verbose = False)

            # Train the discriminator
            d_loss_real = self.discriminator.train_on_batch(imgs, valid)
            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

            # ---------------------
            #  Train Generator
            # ---------------------
            # Train the generator (to have the discriminator label samples as valid)

            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
            g_loss = self.combined.train_on_batch(noise, valid)

            if (epoch % 100) == 0:
              print("%d [D loss: %f, acc.: %.2f%%] [G loss: %f]" % (epoch, d_loss[0], 100*d_loss[1], g_loss))

      print("save model")
      self.generator.save(model_name)

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

data = pd.read_csv('department1.csv', sep = ',', na_values=['(NA)']).fillna(0)
X_train_department1 = data.iloc[:,0:5]

N = len(X_train_department1)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 0.42314
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],3)))

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department1)
train_GAN = scaler0.transform(X_train_department1)
train_GAN_department1 = pd.DataFrame(train_GAN)

#gan_train = GAN(privacy = True, input_dim = 5)
#gan_train.train(data = np.array(train_GAN_department1), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department1_e13.h5")

# Generate a batch of new customers
generator = load_model('train_department1_e13.h5')
noise = np.random.normal(0, 1, (N, 5))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department1 = pd.DataFrame(gen_imgs.reshape(N, 5))
print(train_GAN_department1.head())


# n is sample to speed up
n = 1000

# obtain optimal params
clf = RandomForestClassifier(random_state=0, n_jobs = -1)
param_grid = {
    'class_weight': [None,"balanced", "balanced_subsample"],
    'n_estimators': [50, 100, 200,500],
    'max_features' : ['sqrt', 'log2', None],
    'max_depth': [1,5,None]
}
clf_grid = GridSearchCV(clf, param_grid, scoring = "f1", cv=2, n_jobs = -1)
clf_grid.fit(train_GAN_department1.iloc[:,1:5].sample(n), np.around(train_GAN_department1.iloc[:,0].sample(n)))

print(clf_grid.best_params_)

clf_GAN = clf_grid.best_estimator_
clf_GAN.fit(train_GAN_department1.iloc[:,1:5], np.around(train_GAN_department1.iloc[:,0]))

test = pd.read_csv('test_pred.csv', sep = ',', na_values=['(NA)']).fillna(0).sample(frac = 1)
y_test = test.iloc[:,0]
X_test_department1 = test.iloc[:,1:5]
X_test_department2 = test.iloc[:,5:10]

#predictions
predictions = clf_GAN.predict_proba(X_test_department1)
dataset = pd.DataFrame({'truth': y_test, 'pred': predictions[:,0]}, columns=['truth', 'pred'])
sorted = dataset.sort_values('pred', ascending = False)
top = sorted.nlargest(n = 1000, columns = 'pred')
percentage_real = sum(top['truth'])

print(percentage_real)

"""## firm 1"""

data = pd.read_csv('department1.csv', sep = ',', na_values=['(NA)']).fillna(0)
X_train_department1 = data.iloc[:,0:5]

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)
from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy
from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer, DPKerasAdamOptimizer
from tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy_lib import compute_dp_sgd_privacy
from sklearn.preprocessing import MinMaxScaler

N = len(X_train_department1)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 0.50315 ## vary to get to 6.5, because 6.5*2 = 13.
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],2)))

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department1)
train_GAN = scaler0.transform(X_train_department1)
train_GAN_department1 = pd.DataFrame(train_GAN)

#gan_train = GAN(privacy = True, input_dim = 5)
#gan_train.train(data = np.array(train_GAN_department1), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department1_e6_5.h5")

# Generate a batch of new customers
generator = load_model('train_department1_e6_5.h5')
noise = np.random.normal(0, 1, (N, 5))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department1 = pd.DataFrame(gen_imgs.reshape(N, 5))
print(train_GAN_department1.head())

"""## firm 2"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department2)
train_GAN = scaler0.transform(X_train_department2)
train_GAN_department2 = pd.DataFrame(train_GAN)

N = len(X_train_department2)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 0.50315 ## vary to get to 6.5, because 6.5*2 = 13.
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],2)))

#gan_train = GAN(privacy = True, input_dim = 4)
#gan_train.train(data = np.array(train_GAN_department2), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department2_e6_5.h5")

# Generate a batch of new customers
generator = load_model('train_department2_e6_5.h5')
noise = np.random.normal(0, 1, (N, 4))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department2 = pd.DataFrame(gen_imgs.reshape(N, 4))
print(train_GAN_department2.head())

"""## combine and target"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

concatenated = pd.concat([train_GAN_department1, train_GAN_department2], axis="columns")
concatenated.columns = train.columns
y_train = concatenated.iloc[:,0]
X_train = concatenated.iloc[:,1:100]
print(y_train)
print(X_train)

# n is sample to speed up
n = 1000

# obtain optimal params
clf = RandomForestClassifier(random_state=0, n_jobs = -1)
param_grid = {
    'class_weight': [None,"balanced", "balanced_subsample"],
    'n_estimators': [50, 100, 200, 500],
    'max_features' : ['sqrt', 'log2', None],
    'max_depth': [1,5,None]
}
clf_grid = GridSearchCV(clf, param_grid, scoring = "f1", cv=2, n_jobs = -1)
clf_grid.fit(X_train.sample(n), np.round(y_train.sample(n)))

print(clf_grid.best_params_)

clf_GAN = clf_grid.best_estimator_
clf_GAN.fit(X_train, np.round(y_train))

test = pd.read_csv('test_pred.csv', sep = ',', na_values=['(NA)']).fillna(0).sample(frac = 1)
y_test = test.iloc[:,0]
X_test = test.iloc[:,1:10]

#predictions
predictions = clf_GAN.predict_proba(X_test)
dataset = pd.DataFrame({'truth': y_test, 'pred': predictions[:,1]}, columns=['truth', 'pred'])
sorted = dataset.sort_values('pred', ascending = False)
top = sorted.nlargest(n = 1000, columns = 'pred')
percentage_real = sum(top['truth'])/1000
print(percentage_real*1000)


print(f1_score(y_test, clf_GAN.predict(X_test)))

"""# differential privacy $ɛ = 3$."""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)
from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy
from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer, DPKerasAdamOptimizer
from tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy_lib import compute_dp_sgd_privacy
from sklearn.preprocessing import MinMaxScaler

data = pd.read_csv('department1.csv', sep = ',', na_values=['(NA)']).fillna(0)
X_train_department1 = data.iloc[:,0:5]

N = len(X_train_department1)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 0.6289
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],3)))


scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department1)
train_GAN = scaler0.transform(X_train_department1)
train_GAN_department1 = pd.DataFrame(train_GAN)

#gan_train = GAN(privacy = True, input_dim = 5)
#gan_train.train(data = np.array(train_GAN_department1), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department1_e3.h5")

# Generate a batch of new customers
generator = load_model('train_department1_e3.h5')
noise = np.random.normal(0, 1, (N, 5))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department1 = pd.DataFrame(gen_imgs.reshape(N, 5))
print(train_GAN_department1.head())

# n is sample to speed up
n = 1000

# obtain optimal params
clf = RandomForestClassifier(random_state=0, n_jobs = -1)
param_grid = {
    'class_weight': [None,"balanced", "balanced_subsample"],
    'n_estimators': [50, 100, 200,500],
    'max_features' : ['sqrt', 'log2', None],
    'max_depth': [1,5,None]
}
clf_grid = GridSearchCV(clf, param_grid, scoring = "f1", cv=2, n_jobs = -1)
clf_grid.fit(train_GAN_department1.iloc[:,1:5].sample(n), np.around(train_GAN_department1.iloc[:,0].sample(n)))

print(clf_grid.best_params_)


clf_GAN = clf_grid.best_estimator_
clf_GAN.fit(train_GAN_department1.iloc[:,1:5], np.around(train_GAN_department1.iloc[:,0]))


test = pd.read_csv('test_pred.csv', sep = ',', na_values=['(NA)']).fillna(0)
y_test = test.iloc[:,0]
X_test_department1 = test.iloc[:,1:5]

#predictions
predictions = clf_GAN.predict_proba(X_test_department1)
dataset = pd.DataFrame({'truth': y_test, 'pred': predictions[:,1]}, columns=['truth', 'pred'])
sorted = dataset.sort_values('pred', ascending = False)
top = sorted.nlargest(n = 1000, columns = 'pred')
percentage_real = sum(top['truth'])/1000
print(percentage_real*1000)

"""## firm 1"""

data = pd.read_csv('department1.csv', sep = ',', na_values=['(NA)']).fillna(0)
X_train_department1 = data.iloc[:,0:5]

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)
from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy
from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer, DPKerasAdamOptimizer
from tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy_lib import compute_dp_sgd_privacy
from sklearn.preprocessing import MinMaxScaler

N = len(X_train_department1)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 0.7885 ## vary to get to 6.5, because 6.5*2 = 13.
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],3)))

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department1)
train_GAN = scaler0.transform(X_train_department1)
train_GAN_department1 = pd.DataFrame(train_GAN)

#gan_train = GAN(privacy = True, input_dim = 5)
#gan_train.train(data = np.array(train_GAN_department1), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department1_e1_5.h5")

# Generate a batch of new customers
generator = load_model('train_department1_e1_5.h5')
noise = np.random.normal(0, 1, (N, 5))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department1 = pd.DataFrame(gen_imgs.reshape(N, 5))
print(train_GAN_department1.head())

"""## firm 2"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

X_train_department2 = pd.read_csv('department2.csv', sep = ',', na_values=['(NA)']).fillna(0)
print(X_train_department2)

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department2)
train_GAN = scaler0.transform(X_train_department2)
train_GAN_department2 = pd.DataFrame(train_GAN)

N = len(X_train_department2)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 0.5031 ## vary to get to 6.5, because 6.5*2 = 13.
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],3)))

#gan_train = GAN(privacy = True, input_dim = 4)
#gan_train.train(data = np.array(train_GAN_department2), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department2_e1_5.h5")

# Generate a batch of new customers
generator = load_model('train_department2_e1_5.h5')
noise = np.random.normal(0, 1, (N, 4))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department2 = pd.DataFrame(gen_imgs.reshape(N, 4))
print(train_GAN_department2.head())

"""### combine and target"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

concatenated = pd.concat([train_GAN_department1, train_GAN_department2], axis="columns")
concatenated.columns = train.columns
y_train = concatenated.iloc[:,0]
X_train = concatenated.iloc[:,1:100]
print(y_train)
print(X_train)


# n is sample to speed up
n = 1000

# obtain optimal params
clf = RandomForestClassifier(random_state=0, n_jobs = -1)
param_grid = {
    'class_weight': [None,"balanced", "balanced_subsample"],
    'n_estimators': [50, 100, 200,500],
    'max_features' : ['sqrt', 'log2', None],
    'max_depth': [1,5,None]
}
clf_grid = GridSearchCV(clf, param_grid, scoring = "f1", cv=2, n_jobs = -1)
clf_grid.fit(X_train.sample(n), np.round(y_train.sample(n)))

print(clf_grid.best_params_)

clf_GAN = clf_grid.best_estimator_
clf_GAN.fit(X_train, np.round(y_train))

test = pd.read_csv('test_pred.csv', sep = ',', na_values=['(NA)']).fillna(0).sample(frac = 1)
y_test = test.iloc[:,0]
X_test = test.iloc[:,1:10]

#predictions
predictions = clf_GAN.predict_proba(X_test)
dataset = pd.DataFrame({'truth': y_test, 'pred': predictions[:,1]}, columns=['truth', 'pred'])
sorted = dataset.sort_values('pred', ascending = False)
top = sorted.nlargest(n = 1000, columns = 'pred')
percentage_real = sum(top['truth'])/1000
print(percentage_real*1000)

print(f1_score(y_test, clf_GAN.predict(X_test)))

"""# differential privacy $ɛ = 1$.

## firm 1
"""

data = pd.read_csv('department1.csv', sep = ',', na_values=['(NA)']).fillna(0)
X_train_department1 = data.iloc[:,0:5]

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)
from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy
from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer, DPKerasAdamOptimizer
from tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy_lib import compute_dp_sgd_privacy
from sklearn.preprocessing import MinMaxScaler

N = len(X_train_department1)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 1.245
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],3)))

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department1)
train_GAN = scaler0.transform(X_train_department1)
train_GAN_department1 = pd.DataFrame(train_GAN)

#gan_train = GAN(privacy = True, input_dim = 5)
#gan_train.train(data = np.array(train_GAN_department1), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department1_e0_5.h5")

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)
# Generate a batch of new customers
generator = load_model('train_department1_e0_5.h5')
noise = np.random.normal(0, 1, (N, 5))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department1 = pd.DataFrame(gen_imgs.reshape(N, 5))
print(train_GAN_department1.head())

"""## firm 2"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

X_train_department2 = pd.read_csv('department2.csv', sep = ',', na_values=['(NA)']).fillna(0)
print(X_train_department2)

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department2)
train_GAN = scaler0.transform(X_train_department2)
train_GAN_department2 = pd.DataFrame(train_GAN)

N = len(X_train_department2)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 1.245 ## vary to get to 6.5, because 6.5*2 = 13.
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],3)))

# train model
#gan_train = GAN(privacy = True, input_dim = 4)
#gan_train.train(data = np.array(train_GAN_department2), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department2_e0_5.h5")

# Generate a batch of new customers
generator = load_model('train_department2_e0_5.h5')
noise = np.random.normal(0, 1, (N, 4))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department2 = pd.DataFrame(gen_imgs.reshape(N, 4))
print(train_GAN_department2.head())

"""## combine and target"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

concatenated = pd.concat([train_GAN_department1, train_GAN_department2], axis="columns")
concatenated.columns = train.columns
y_train = concatenated.iloc[:,0]
X_train = concatenated.iloc[:,1:100]
print(y_train)
print(X_train)

# n is sample to speed up
n = 1000

# obtain optimal params
clf = RandomForestClassifier(random_state=0, n_jobs = -1)
param_grid = {
    'class_weight': [None,"balanced", "balanced_subsample"],
    'n_estimators': [50, 100, 200,500],
    'max_features' : ['sqrt', 'log2', None],
    'max_depth': [1,5,None]
}
clf_grid = GridSearchCV(clf, param_grid, scoring = "f1", cv=2, n_jobs = -1)
clf_grid.fit(X_train.sample(n), np.round(y_train.sample(n)))

print(clf_grid.best_params_)

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

# use best model
clf_GAN = clf_grid.best_estimator_
clf_GAN.fit(X_train, np.round(y_train))

test = pd.read_csv('test_pred.csv', sep = ',', na_values=['(NA)']).fillna(0).sample(frac = 1)
y_test = test.iloc[:,0]
X_test = test.iloc[:,1:10]

#predictions
predictions = clf_GAN.predict_proba(X_test)
dataset = pd.DataFrame({'truth': y_test, 'pred': predictions[:,1]}, columns=['truth', 'pred'])
sorted = dataset.sort_values('pred', ascending = False)
top = sorted.nlargest(n = 1000, columns = 'pred')
percentage_real = sum(top['truth'])/1000
print(percentage_real*1000)

print(f1_score(y_test, clf_GAN.predict(X_test)))

"""# differential privacy $ɛ = 0.5$.

## firm 1
"""

data = pd.read_csv('department1.csv', sep = ',', na_values=['(NA)']).fillna(0)
X_train_department1 = data.iloc[:,0:5]

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)
from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy
from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer, DPKerasAdamOptimizer
from tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy_lib import compute_dp_sgd_privacy
from sklearn.preprocessing import MinMaxScaler

N = len(X_train_department1)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 1.8
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],3)))

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department1)
train_GAN = scaler0.transform(X_train_department1)
train_GAN_department1 = pd.DataFrame(train_GAN)

#gan_train = GAN(privacy = True, input_dim = 5)
#gan_train.train(data = np.array(train_GAN_department1), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department1_e0_25.h5")

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)
# Generate a batch of new customers
generator = load_model('train_department1_e0_25.h5')
noise = np.random.normal(0, 1, (N, 5))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department1 = pd.DataFrame(gen_imgs.reshape(N, 5))
print(train_GAN_department1.head())

"""## firm 2"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

X_train_department2 = pd.read_csv('department2.csv', sep = ',', na_values=['(NA)']).fillna(0)
print(X_train_department2)

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department2)
train_GAN = scaler0.transform(X_train_department2)
train_GAN_department2 = pd.DataFrame(train_GAN)

N = len(X_train_department2)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 1.8 ## vary to get to 6.5, because 6.5*2 = 13.
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],3)))

# train model
#gan_train = GAN(privacy = True, input_dim = 4)
#gan_train.train(data = np.array(train_GAN_department2), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department2_e0_25.h5")

# Generate a batch of new customers
generator = load_model('train_department2_e0_25.h5')
noise = np.random.normal(0, 1, (N, 4))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department2 = pd.DataFrame(gen_imgs.reshape(N, 4))
print(train_GAN_department2.head())

"""## combine and target"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

concatenated = pd.concat([train_GAN_department1, train_GAN_department2], axis="columns")
concatenated.columns = train.columns
y_train = concatenated.iloc[:,0]
X_train = concatenated.iloc[:,1:100]
print(y_train)
print(X_train)

# n is sample to speed up
n = 1000

# obtain optimal params
clf = RandomForestClassifier(random_state=0, n_jobs = -1)
param_grid = {
    'class_weight': [None,"balanced", "balanced_subsample"],
    'n_estimators': [50, 100, 200,500],
    'max_features' : ['sqrt', 'log2', None],
    'max_depth': [1,5,None]
}
clf_grid = GridSearchCV(clf, param_grid, scoring = "f1", cv=2, n_jobs = -1)
clf_grid.fit(X_train.sample(n), np.round(y_train.sample(n)))

print(clf_grid.best_params_)

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

# use best model
clf_GAN = clf_grid.best_estimator_
clf_GAN.fit(X_train, np.round(y_train))

test = pd.read_csv('test_pred.csv', sep = ',', na_values=['(NA)']).fillna(0).sample(frac = 1)
y_test = test.iloc[:,0]
X_test = test.iloc[:,1:10]

#predictions
predictions = clf_GAN.predict_proba(X_test)
dataset = pd.DataFrame({'truth': y_test, 'pred': predictions[:,1]}, columns=['truth', 'pred'])
sorted = dataset.sort_values('pred', ascending = False)
top = sorted.nlargest(n = 1000, columns = 'pred')
percentage_real = sum(top['truth'])/1000
print(percentage_real*1000)

print(f1_score(y_test, clf_GAN.predict(X_test)))

"""# differential privacy $ɛ = 0.05$.

## firm 1
"""

data = pd.read_csv('department1.csv', sep = ',', na_values=['(NA)']).fillna(0)
X_train_department1 = data.iloc[:,0:5]

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

N = len(X_train_department1)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 13.3
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],3)))

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department1)
train_GAN = scaler0.transform(X_train_department1)
train_GAN_department1 = pd.DataFrame(train_GAN)

#gan_train = GAN(privacy = True, input_dim = 5)
#gan_train.train(data = np.array(train_GAN_department1), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department1_e0_025.h5")

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)
# Generate a batch of new customers
generator = load_model('train_department1_e0_025.h5')
noise = np.random.normal(0, 1, (N, 5))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department1 = pd.DataFrame(gen_imgs.reshape(N, 5))
print(train_GAN_department1.head())

"""## firm 2"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

X_train_department2 = pd.read_csv('department2.csv', sep = ',', na_values=['(NA)']).fillna(0)
print(X_train_department2)

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department2)
train_GAN = scaler0.transform(X_train_department2)
train_GAN_department2 = pd.DataFrame(train_GAN)

N = len(X_train_department2)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 13.3 ## vary to get to 6.5, because 6.5*2 = 13.
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],3)))

# train model
#gan_train = GAN(privacy = True, input_dim = 4)
#gan_train.train(data = np.array(train_GAN_department2), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department2_e0_025.h5")

# Generate a batch of new customers
generator = load_model('train_department2_e0_025.h5')
noise = np.random.normal(0, 1, (N, 4))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department2 = pd.DataFrame(gen_imgs.reshape(N, 4))
print(train_GAN_department2.head())

"""## combine and target"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

concatenated = pd.concat([train_GAN_department1, train_GAN_department2], axis="columns")
concatenated.columns = train.columns
y_train = concatenated.iloc[:,0]
X_train = concatenated.iloc[:,1:100]
print(y_train)
print(X_train)

# n is sample to speed up
n = 1000

# obtain optimal params
clf = RandomForestClassifier(random_state=0, n_jobs = -1)
param_grid = {
    'class_weight': [None,"balanced", "balanced_subsample"],
    'n_estimators': [100, 200,500],
    'max_features' : ['sqrt', 'log2', None],
    'max_depth': [1,5,None]
}
clf_grid = GridSearchCV(clf, param_grid, scoring = "f1", cv=2, n_jobs = -1)
clf_grid.fit(X_train.sample(n), np.round(y_train.sample(n)))

print(clf_grid.best_params_)

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

# use best model
clf_GAN = clf_grid.best_estimator_
clf_GAN.fit(X_train, np.round(y_train))

test = pd.read_csv('test_pred.csv', sep = ',', na_values=['(NA)']).fillna(0).sample(frac = 1)
y_test = test.iloc[:,0]
X_test = test.iloc[:,1:10]

#predictions
predictions = clf_GAN.predict_proba(X_test)
dataset = pd.DataFrame({'truth': y_test, 'pred': predictions[:,0]}, columns=['truth', 'pred'])
sorted = dataset.sort_values('pred', ascending = False)
top = sorted.nlargest(n = 1000, columns = 'pred')
percentage_real = sum(top['truth'])/1000
print(percentage_real*1000)

print(f1_score(y_test, clf_GAN.predict(X_test)))

"""# differential privacy $ɛ = 0.01$.

## firm 1
"""

data = pd.read_csv('department1.csv', sep = ',', na_values=['(NA)']).fillna(0)
X_train_department1 = data.iloc[:,0:5]

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

N = len(X_train_department1)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 1000
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],3)))

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department1)
train_GAN = scaler0.transform(X_train_department1)
train_GAN_department1 = pd.DataFrame(train_GAN)

gan_train = GAN(privacy = True, input_dim = 5)
gan_train.train(data = np.array(train_GAN_department1), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department1_e0_005.h5")

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)
# Generate a batch of new customers
generator = load_model('train_department1_e0_005.h5')
noise = np.random.normal(0, 1, (N, 5))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department1 = pd.DataFrame(gen_imgs.reshape(N, 5))
print(train_GAN_department1.head())

"""## firm 2"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

X_train_department2 = pd.read_csv('department2.csv', sep = ',', na_values=['(NA)']).fillna(0)
print(X_train_department2)

scaler0 = MinMaxScaler(feature_range= (-1, 1))
scaler0 = scaler0.fit(X_train_department2)
train_GAN = scaler0.transform(X_train_department2)
train_GAN_department2 = pd.DataFrame(train_GAN)

N = len(X_train_department2)
iterations = 10000
batch_size = 100
epochs = iterations/(N/batch_size)
print(epochs)

noise_multiplier = 1000 ## vary to get to 6.5, because 6.5*2 = 13.
l2_norm_clip = 4 # see abadi et al 2016
delta= 1/N
theor_epsilon =compute_dp_sgd_privacy(N, batch_size, noise_multiplier,epochs, delta)
num_microbatches = batch_size
print("theoretical epsilon = " + str(round(theor_epsilon[0],3)))

# train model
gan_train = GAN(privacy = True, input_dim = 4)
gan_train.train(data = np.array(train_GAN_department2), iterations=iterations, batch_size=batch_size, sample_interval=100, model_name = "train_department2_e0_005.h5")

# Generate a batch of new customers
generator = load_model('train_department2_e0_005.h5')
noise = np.random.normal(0, 1, (N, 4))
gen_imgs = generator.predict(noise)
#print(gen_imgs)
gen_imgs = scaler0.inverse_transform(gen_imgs)
train_GAN_department2 = pd.DataFrame(gen_imgs.reshape(N, 4))
print(train_GAN_department2.head())

"""## combine and target"""

random.seed(1)
np.random.seed(1)
tf.random.set_seed(1)

concatenated = pd.concat([train_GAN_department1, train_GAN_department2], axis="columns")
concatenated.columns = train.columns
y_train = concatenated.iloc[:,0]
X_train = concatenated.iloc[:,1:100]
print(y_train)
print(X_train)

# n is sample to speed up
n = 1000

# obtain optimal params
clf = RandomForestClassifier(random_state=0, n_jobs = -1)
param_grid = {
    'class_weight': [None,"balanced", "balanced_subsample"],
    'n_estimators': [100, 200,500],
    'max_features' : ['sqrt', 'log2', None],
    'max_depth': [1,5,None]
}
clf_grid = GridSearchCV(clf, param_grid, scoring = "f1", cv=2, n_jobs = -1)
clf_grid.fit(X_train.sample(n), np.round(y_train.sample(n)))

print(clf_grid.best_params_)

# use best model
clf_GAN = clf_grid.best_estimator_
clf_GAN.fit(X_train, np.round(y_train))

test = pd.read_csv('test_pred.csv', sep = ',', na_values=['(NA)']).fillna(0).sample(frac = 1)
y_test = test.iloc[:,0]
X_test = test.iloc[:,1:10]

#predictions
predictions = clf_GAN.predict_proba(X_test)
dataset = pd.DataFrame({'truth': y_test, 'pred': predictions[:,0]}, columns=['truth', 'pred'])
sorted = dataset.sort_values('pred', ascending = False)
top = sorted.nlargest(n = 1000, columns = 'pred')
percentage_real = sum(top['truth'])/1000
print(percentage_real*1000)

print(f1_score(y_test, clf_GAN.predict(X_test)))